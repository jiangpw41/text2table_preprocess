{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wayne_utils import load_data, save_data, _PUNCATUATION_EN, _PUNCATUATION_ZH\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "_ROOT_PATH = \"/home/jiangpeiwen2/jiangpeiwen2/text2table_preprocess/CPL\"\n",
    "\n",
    "# 列表出所有的文本和表格路径\n",
    "first_data_path = os.path.join( _ROOT_PATH, \"raw/FirstCollection\")\n",
    "second_data_path = os.path.join( _ROOT_PATH, \"raw/SecondCollection\")\n",
    "first_doc_file_list = os.listdir( os.path.join(first_data_path, \"doc\") )\n",
    "first_excel_file_list = os.listdir( os.path.join(first_data_path, \"excel\") )\n",
    "second_doc_file_list = os.listdir( os.path.join(second_data_path, \"doc\") )\n",
    "second_excel_file_list = os.listdir( os.path.join(second_data_path, \"excel\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取、排序、对齐为850个pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理第0次采集的数据集: 100%|██████████| 391/391 [00:11<00:00, 34.29it/s]\n",
      "处理第1次采集的数据集: 100%|██████████| 459/459 [00:11<00:00, 41.45it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_sorted_file_names( file_list, target_path ):\n",
    "    \"\"\"对文件名列表根据文件名进行排序\n",
    "    param: file_list, 文件列表\n",
    "    param: target_path, 根目录，可与文件名组合成完整路径\n",
    "    output: list_sorted, 排序完毕的返回文件名列表\n",
    "    \"\"\"\n",
    "    # 用.分隔符获取每个文件名的编号并Int化\n",
    "    split_list = []\n",
    "    for i in range(len(file_list)):\n",
    "        temp = file_list[i].split(\".\")\n",
    "        temp[0] = int(temp[0])\n",
    "        split_list.append( temp )\n",
    "    # 根据文件名编号对整个列表进行排序，并将编号变回字符串并与文件名合并\n",
    "    list_sorted = sorted(split_list, key=lambda x: x[0])\n",
    "    for i in range(len(list_sorted)):\n",
    "        list_sorted[i][0] = str(list_sorted[i][0])\n",
    "        list_sorted[i] = \".\".join(list_sorted[i])\n",
    "    # 对排序重组后的列表中的每个文件名进行存在性检查\n",
    "    for i in range(len(list_sorted)):\n",
    "        total_path = os.path.join(target_path, list_sorted[i] )\n",
    "        if not os.path.exists(total_path):\n",
    "            raise Exception(f\"Some problems on {i}: {total_path}\")\n",
    "    return list_sorted\n",
    "\n",
    "def get_file_name_pairs():\n",
    "    \"\"\"获取文本、表格文件名对\"\"\"\n",
    "    sorted_first_doc_list = get_sorted_file_names(first_doc_file_list, os.path.join(first_data_path, \"doc\") )\n",
    "    sorted_first_excel_list = get_sorted_file_names(first_excel_file_list, os.path.join(first_data_path, \"excel\") )\n",
    "    sorted_second_doc_list = get_sorted_file_names(second_doc_file_list, os.path.join(second_data_path, \"doc\") )\n",
    "    sorted_second_excel_list = get_sorted_file_names(second_excel_file_list, os.path.join(second_data_path, \"excel\") )\n",
    "\n",
    "    text_data_pairs_1 = []\n",
    "    for i in range( len(sorted_first_doc_list) ):\n",
    "        text_data_pairs_1.append( (sorted_first_doc_list[i], sorted_first_excel_list[i]) )\n",
    "    text_data_pairs_2 = []\n",
    "    for i in range( len(sorted_second_doc_list) ):\n",
    "        text_data_pairs_2.append( (sorted_second_doc_list[i], sorted_second_excel_list[i]) )\n",
    "    return text_data_pairs_1, text_data_pairs_2\n",
    "\n",
    "def get_data_pair( ):\n",
    "    \"\"\"获取两个子集850分数据对\"\"\"\n",
    "    text_data_pairs_1, text_data_pairs_2 = get_file_name_pairs()\n",
    "    file_name_pair = []\n",
    "    data_pair = []\n",
    "    for i, lists in enumerate( [text_data_pairs_1, text_data_pairs_2]):\n",
    "        \n",
    "        prefix_path = os.path.join(_ROOT_PATH, f\"raw/FirstCollection\") if i == 0 else os.path.join(_ROOT_PATH, f\"raw/SecondCollection\")\n",
    "        for j in tqdm( range(len(lists)), desc = f\"处理第{i}次采集的数据集\"):\n",
    "            file_name_pair.append( (lists[j][0], lists[j][1]))\n",
    "            text = load_data( os.path.join( prefix_path, f\"doc/{lists[j][0]}\" ), \"docx\")\n",
    "            table = load_data( os.path.join( prefix_path, f\"excel/{lists[j][1]}\"), \"excel\")\n",
    "            data_pair.append( (text, table))\n",
    "    return data_pair, file_name_pair\n",
    "\n",
    "data_pair, file_name_pair = get_data_pair( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过滤非单笔交易，剩下703份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_court_name( text_list ):\n",
    "    \"从文本获取法庭名\"\n",
    "    court_name = {}\n",
    "    no_name = []\n",
    "    for i in range(len(text_list)):\n",
    "        splits = text_list[i].split(\"\\\\n\")\n",
    "        flag=0\n",
    "        for j in range(9):\n",
    "            if \"法院\" in splits[j]:\n",
    "                name = splits[j]\n",
    "                if \"审理法院：\" in name:\n",
    "                    name = name[5:]\n",
    "                elif \"民事判决书\" in name:\n",
    "                    name = name[:-5]\n",
    "                elif \"中华人民共和国\" in name:\n",
    "                    name = name[7:]\n",
    "                \n",
    "                if name.startswith(\"合肥市\"):\n",
    "                    name = \"安徽省\"+name\n",
    "                court_name[i] = name\n",
    "                flag=1\n",
    "                break\n",
    "        if flag==0:\n",
    "            no_name.append( i )\n",
    "    court_name[561] = \"浙江省温州市中级人民法院\"\n",
    "    court_name[562] = \"安徽省合肥市中级人民法院\"\n",
    "    court_name[793] = \"浙江省温州市中级人民法院\"\n",
    "    return court_name\n",
    "\n",
    "def get_names_from_excel( table ):\n",
    "    \"\"\"从表格中获取entity名称\"\"\"\n",
    "    role_map = {\n",
    "        \"出借人（原告）姓名或名称\": (0, \"出借人（原告）\"),\n",
    "        \"借款人（被告）姓名或名称\": (1, \"借款人（被告）\"),\n",
    "        \"担保人（被告）姓名或名称\": (2, \"担保人（被告）\"),\n",
    "        \"【其他诉讼参与人】姓名或名称\": (3, \"其他诉讼参与人\")\n",
    "        }\n",
    "    # 除了法院，其他都有“：”：[703, 695, 121, 55]\n",
    "    count = [0, 0, 0, 0]\n",
    "    ret = []\n",
    "    for j in range(2, len(table.iloc[0]), 3 ):\n",
    "        if j ==2:\n",
    "            ret.append( (\"法院\", table.iloc[0, j+1]))\n",
    "        else:\n",
    "            fields = table.iloc[0, j].split(\"：\")\n",
    "            filed_name = role_map[fields[0]][1]\n",
    "            field_index = role_map[fields[0]][0]\n",
    "            if fields[1]!=\"\":\n",
    "                # 说明名字和字段在一起了\n",
    "                ret.append( (filed_name, fields[1]))\n",
    "                count[field_index] +=1\n",
    "            elif not pd.isna(table.iloc[0, j+1]):\n",
    "                # 如果名字不在一起但在后一格\n",
    "                ret.append( (filed_name, table.iloc[0, j+1]))\n",
    "                count[field_index] +=1\n",
    "    return ret, count\n",
    "\n",
    "def single_lending_filter( data_pair, file_name_pair ):\n",
    "    \"\"\"过滤其中147份没有原告姓名的（说明不是单次单笔），剩下703份\"\"\"\n",
    "    filter_pairs = []\n",
    "    filter_pairs_names = []\n",
    "    multi_lending_index = []\n",
    "    # 法院列2，原告列5，被告列8，担保11，其他14\n",
    "    for i in range(len(data_pair)):\n",
    "        table = data_pair[i][1]\n",
    "        \n",
    "        if not pd.isna(table.iloc[0, 6]):                   # 原告姓名与字段分开的638\n",
    "            # 如果原告名在相邻格\n",
    "            filter_pairs.append( data_pair[i] )\n",
    "            filter_pairs_names.append( file_name_pair[i] )\n",
    "        else:\n",
    "            # 判断是否在同一格\n",
    "            string_name = table.iloc[0, 5].split(\"：\")      # 没有原告姓名的147\n",
    "            if string_name[1]!=\"\":\n",
    "                # 原告姓名与字段一个格子的65\n",
    "                filter_pairs.append( data_pair[i] )\n",
    "                filter_pairs_names.append( file_name_pair[i] )\n",
    "            else:\n",
    "                multi_lending_index.append( i )\n",
    "                \n",
    "    return filter_pairs, multi_lending_index, filter_pairs_names\n",
    "\n",
    "filter_pairs, multi_lending_index, filter_pairs_names =  single_lending_filter( data_pair, file_name_pair )      # 703, 143，万一以后要处理非单笔可以直接索引\n",
    "# first_column, count = get_names_from_excel( filter_pairs[0][1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filter_pairs_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理并保存文本和表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理并保存文本\n",
    "def docx_processor( text_table_pair, filter_pairs_names, save_path ):\n",
    "    text_list = []\n",
    "    with open( save_path, 'w') as file:\n",
    "        for i in range( len(text_table_pair) ):\n",
    "            text = text_table_pair[i][0]\n",
    "            text = text.replace(\" \", \"\").replace(\"\\u3000\", \"\").replace(\"\\n\\n\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\").replace(\"\\n\", \"\\\\n\") # 特殊字符串清洗\n",
    "            for p in range( len(_PUNCATUATION_EN) ):                                # 英字符转中\n",
    "                if _PUNCATUATION_EN[p] != \".\":\n",
    "                    text = text.replace(_PUNCATUATION_EN[p], _PUNCATUATION_ZH[p])\n",
    "            if i<len(text_table_pair)-1:\n",
    "                file.write(filter_pairs_names[i][0]+\"###\"+text + '\\n')\n",
    "            else:\n",
    "                file.write(filter_pairs_names[i][0]+\"###\"+text )   \n",
    "            text_list.append( text )\n",
    "    return text_list\n",
    "text_save_path = os.path.join(_ROOT_PATH, f\"pairs/texts.text\")\n",
    "text_list = docx_processor( filter_pairs, filter_pairs_names, text_save_path )      # 703"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constructing json: 100%|██████████| 703/703 [00:36<00:00, 19.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# 处理表格\n",
    "from tools.excel_reader import get_FirstColumn, get_DataCell\n",
    "def excel_processor( text_table_pair, json_data):\n",
    "    new_table_list = {}\n",
    "    new_table_split_index_list = {}\n",
    "    for i in tqdm( range(len(text_table_pair)), desc=f\"Constructing json\"):\n",
    "        entity = json_data[str(i)]\n",
    "        table = text_table_pair[i][1]\n",
    "        single_dict = {}\n",
    "        single_dict[ \"first_column\"] = get_FirstColumn( entity )\n",
    "        single_dict[ \"data_cell\"] = get_DataCell( single_dict[ \"first_column\"], table)\n",
    "        new_table_list[i] = single_dict\n",
    "        # new_table_split_index_list[i] = split_index\n",
    "    save_data( new_table_list, os.path.join(_ROOT_PATH, f\"pairs/tables_total.json\") )\n",
    "    return new_table_list # , new_table_split_index_list\n",
    "\n",
    "entity_json_data = load_data( os.path.join( _ROOT_PATH, \"raw/entity_name_703.json\"), \"json\")\n",
    "new_table_list = excel_processor( filter_pairs, entity_json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TKGT-preprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
